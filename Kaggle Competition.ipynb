{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre - processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(csv_file, train = 1):\n",
    "    \n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    data = data.drop('replyToSID', 1)\n",
    "    data = data.drop('truncated', 1)\n",
    "    data = data.drop('screenName', 1)\n",
    "    \n",
    "    if train:\n",
    "        data = data.drop('statusSource', 1)\n",
    "        \n",
    "    data = data.drop('favorited', 1)\n",
    "    data = data.drop('replyToSN', 1)\n",
    "    data = data.drop('replyToUID', 1)\n",
    "    data = data.drop('isRetweet', 1)\n",
    "    data = data.drop('retweeted', 1)\n",
    "    data = data.drop('id', 1)\n",
    "    data = data.drop('id.1', 1)\n",
    "    data = data.drop('latitude', 1)\n",
    "    data = data.drop('longitude', 1)\n",
    "    data = data.drop('favoriteCount', 1)\n",
    "    data = data.drop('retweetCount', 1)\n",
    "    \n",
    "    data.created = pd.to_datetime(data.created)\n",
    "    \n",
    "    data['textLength'] = data.text.apply(len)\n",
    "\n",
    "    data['textSentiment'] = data.text.apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "    \n",
    "    data['createdHour'] = data.created.apply(lambda x: x.hour)\n",
    "    data['createdDay'] = data.created.apply(lambda x: x.dayofweek)\n",
    "    \n",
    "    \n",
    "    data = data.drop('text', 1)\n",
    "    data = data.drop('created', 1)\n",
    "    \n",
    "    \n",
    "    data.createdDay = pd.Categorical(data.createdDay)\n",
    "    data.createdHour = pd.Categorical(data.createdHour)\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    if train:\n",
    "        labels = data.label\n",
    "        data = data.drop('label', 1)\n",
    "    \n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, trainLabels = cleanData('train.csv', train = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData, testLabels = cleanData('test.csv', train = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.7171083927964662 0.06281410165092617\n",
      "200 0.7088515120625213 0.05821494937441702\n",
      "300 0.7070166496772001 0.05926520381313302\n",
      "400 0.7134386680258239 0.062256789135299785\n",
      "500 0.7134641522256201 0.061004516933901715\n",
      "600 0.716199456337071 0.0628413755778691\n",
      "700 0.7143645939517499 0.0583884395707241\n",
      "800 0.7079425756031261 0.05514277370413377\n",
      "900 0.7161909616038057 0.06865473695774138\n",
      "1000 0.7070251444104655 0.05462376978545508\n",
      "1100 0.7143560992184846 0.06434241155457426\n",
      "1200 0.7134471627590895 0.061930323149715555\n",
      "1300 0.7180343187223921 0.05949329425346044\n",
      "1400 0.7116123003737683 0.06129255217988205\n",
      "1500 0.713438668025824 0.05688788094624898\n",
      "1600 0.7134471627590894 0.06069483113610427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-204-693a742ebb4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlabels_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mclfRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclfRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 tree = self._make_estimator(append=False,\n\u001b[1;32m--> 314\u001b[1;33m                                             random_state=random_state)\n\u001b[0m\u001b[0;32m    315\u001b[0m                 \u001b[0mtrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\base.pyc\u001b[0m in \u001b[0;36m_make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[0m_set_random_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mappend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\base.pyc\u001b[0m in \u001b[0;36m_set_random_states\u001b[1;34m(estimator, random_state)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mto_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'random_state'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__random_state'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mto_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_RAND_SEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mget_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"always\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                 \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Raghav\\Anaconda3\\lib\\warnings.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_entered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showwarning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowwarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "trainData = np.array(trainData)\n",
    "\n",
    "for i in range(1, 20):\n",
    "    \n",
    "    clfRF = RandomForestClassifier(n_estimators=i * 100, criterion=\"entropy\")\n",
    "\n",
    "    accuracy_lst = []\n",
    "\n",
    "    for train_index, test_index in kf.split(trainData):\n",
    "\n",
    "        data_train = trainData[train_index]\n",
    "        data_test = trainData[test_index]\n",
    "        labels_train = labels[train_index]\n",
    "        labels_test = labels[test_index]\n",
    "\n",
    "        clfRF.fit(data_train, labels_train)\n",
    "\n",
    "        accuracy = np.sum(clfRF.predict(data_test) == labels_test) * 1.0/len(data_test)\n",
    "\n",
    "        accuracy_lst.append(accuracy) \n",
    "        \n",
    "    print i * 100, np.mean(accuracy_lst), np.std(accuracy_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfRF = RandomForestClassifier(n_estimators=1600, criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1600, n_jobs=1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfRF.fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfRF_probs = clfRF.predict_proba(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.max(clfRF_probs, axis = 1) < 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clfRF.predict(testData)).to_csv('predictions_less_RF.csv', header=['Label'], index_label = ['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 0.7207696228338429 0.05179453430178334\n",
      "10 2 0.7033299354400271 0.05285607346459183\n",
      "10 3 0.7024379884471628 0.050721375134452434\n",
      "10 4 0.6987597689432551 0.052970881705554015\n",
      "10 5 0.6886595310907238 0.04860660023185316\n",
      "10 6 0.6739806320081548 0.04741842666107704\n",
      "10 7 0.6730377166156982 0.04970359826246403\n",
      "10 8 0.6721287801563032 0.05126575466870997\n",
      "10 9 0.6730462113489637 0.05227182122570788\n",
      "10 10 0.6693764865783214 0.058451671964495536\n",
      "10 11 0.663888888888889 0.05506503446600879\n",
      "10 12 0.6666496772001359 0.05941439923287854\n",
      "10 13 0.6629799524294937 0.058249745612893866\n",
      "10 14 0.6620625212368332 0.05674412896519435\n",
      "10 15 0.6620625212368331 0.055240928425989516\n",
      "10 16 0.6648063200815495 0.0543087431396132\n",
      "10 17 0.6593017329255861 0.05492261301930248\n",
      "10 18 0.6583673122663948 0.0576315416782045\n",
      "10 19 0.6556150186884132 0.05826136760858505\n",
      "11 1 0.7198521916411824 0.05355443195530491\n",
      "11 2 0.7051647978253482 0.055265259190053435\n",
      "11 3 0.7033554196398233 0.05265655477332405\n",
      "11 4 0.6978423377505946 0.05322684900605143\n",
      "11 5 0.6886595310907238 0.04684298866901936\n",
      "11 6 0.6739721372748896 0.04940150090419364\n",
      "11 7 0.6767244308528714 0.04487720551682559\n",
      "11 8 0.6684675501189263 0.05301389825911053\n",
      "11 9 0.6712113489636425 0.05022278914263739\n",
      "11 10 0.6703109072375127 0.05738576111419098\n",
      "11 11 0.6684760448521916 0.060097822057924986\n",
      "11 12 0.6648148148148149 0.05914884419656137\n",
      "11 13 0.663888888888889 0.057896428371567346\n",
      "11 14 0.6620625212368332 0.05718738472438309\n",
      "11 15 0.6638973836221542 0.05546450937161484\n",
      "11 16 0.6611365953109072 0.05242491376257121\n",
      "11 17 0.6611281005776419 0.0570963472710631\n",
      "11 18 0.6592847434590554 0.058167656726425744\n",
      "11 19 0.6556065239551477 0.057888098890806094\n",
      "12 1 0.7207781175671084 0.05106883580273692\n",
      "12 2 0.7060907237512742 0.05117694973371664\n",
      "12 3 0.7033554196398233 0.0510330947713615\n",
      "12 4 0.696924906557934 0.046746692363871266\n",
      "12 5 0.6859072375127422 0.046162748054594915\n",
      "12 6 0.6748895684675501 0.04493829193040359\n",
      "12 7 0.6730462113489637 0.0462943549362746\n",
      "12 8 0.6730632008154943 0.050830400591871806\n",
      "12 9 0.673054706082229 0.05252505632908348\n",
      "12 10 0.6675586136595311 0.05771361103200105\n",
      "12 11 0.6666411824668705 0.06128051415382861\n",
      "12 12 0.6666411824668705 0.05960954803195218\n",
      "12 13 0.6629714576962285 0.058878677781234276\n",
      "12 14 0.6620625212368332 0.05791860643127784\n",
      "12 15 0.6620625212368332 0.05629738335324843\n",
      "12 16 0.6602106693849813 0.05672853854424681\n",
      "12 17 0.6611281005776419 0.05840800690389818\n",
      "12 18 0.65927624872579 0.057502317114598675\n",
      "12 19 0.6583673122663949 0.05585151204598356\n",
      "13 1 0.7226044852191642 0.055204123945770914\n",
      "13 2 0.7097689432551818 0.050167225067538936\n",
      "13 3 0.7015205572545022 0.053940660601884426\n",
      "13 4 0.6941726129799524 0.047451580020387366\n",
      "13 5 0.683146449201495 0.04808403226936345\n",
      "13 6 0.6776418620455319 0.04772874269822656\n",
      "13 7 0.6721372748895685 0.04814622975097119\n",
      "13 8 0.671219843696908 0.048444760009065516\n",
      "13 9 0.6730632008154943 0.056626608757256655\n",
      "13 10 0.6703109072375127 0.05840343121523181\n",
      "13 11 0.6684845395854571 0.060184357287702724\n",
      "13 12 0.6684845395854571 0.05761208262645862\n",
      "13 13 0.6602191641182467 0.06137620202377868\n",
      "13 14 0.6611450900441727 0.05887203170196453\n",
      "13 15 0.6611365953109074 0.05644506337592012\n",
      "13 16 0.6611365953109072 0.057332772292150905\n",
      "13 17 0.6629544682296975 0.05485398677737805\n",
      "13 18 0.6620285423037716 0.05578860954622412\n",
      "13 19 0.6574498810737344 0.056927989104816244\n",
      "14 1 0.72353041114509 0.05385334221670603\n",
      "14 2 0.7088515120625212 0.0522730325877313\n",
      "14 3 0.7024379884471628 0.05409402455633898\n",
      "14 4 0.6950900441726129 0.044671381686864346\n",
      "14 5 0.6813030920829085 0.046871041577246336\n",
      "14 6 0.6822460074753652 0.04540163074847254\n",
      "14 7 0.6721457696228338 0.04859794137724274\n",
      "14 8 0.6721372748895685 0.0477953160833009\n",
      "14 9 0.6739806320081551 0.057524116494978066\n",
      "14 10 0.6693934760448522 0.058188866442923995\n",
      "14 11 0.6694019707781177 0.0556177349385768\n",
      "14 12 0.6666496772001359 0.05606155580521521\n",
      "14 13 0.6611365953109072 0.06047632574603811\n",
      "14 14 0.6620455317703025 0.05960435635105048\n",
      "14 15 0.6611365953109072 0.057332772292150905\n",
      "14 16 0.6629714576962285 0.05654521287802454\n",
      "14 17 0.6629544682296975 0.05485398677737805\n",
      "14 18 0.6602021746517159 0.05559108763751158\n",
      "14 19 0.6574498810737344 0.057222924979843\n",
      "15 1 0.7226129799524295 0.053432023496569246\n",
      "15 2 0.7060992184845396 0.050444838957224576\n",
      "15 3 0.7015205572545022 0.05578170008660239\n",
      "15 4 0.695090044172613 0.04669798646978435\n",
      "15 5 0.6785507985049269 0.04946351884508416\n",
      "15 6 0.6794852191641183 0.045315758722235436\n",
      "15 7 0.672145769622834 0.04842443914224087\n",
      "15 8 0.6739806320081551 0.046522458960004336\n",
      "15 9 0.6748980632008156 0.05953537248262008\n",
      "15 10 0.6675501189262658 0.0608927129858825\n",
      "15 11 0.6675671083927964 0.05450638434174671\n",
      "15 12 0.6666496772001358 0.0565101658013605\n",
      "15 13 0.663888888888889 0.058905276650240306\n",
      "15 14 0.6592932381923208 0.05810835686398124\n",
      "15 15 0.6620540265035677 0.05605393848663669\n",
      "15 16 0.6620455317703025 0.055661226178185\n",
      "15 17 0.6647978253482841 0.054215141989394366\n",
      "15 18 0.6602021746517159 0.05678941647564397\n",
      "15 19 0.65927624872579 0.05793977280887336\n",
      "16 1 0.7216955487597689 0.052191267877423876\n",
      "16 2 0.7051732925586137 0.05157242115435616\n",
      "16 3 0.7015205572545022 0.05578170008660239\n",
      "16 4 0.6923377505946313 0.04835289628423262\n",
      "16 5 0.6758069996602106 0.04817212166679\n",
      "16 6 0.6748810737342847 0.044642390245530454\n",
      "16 7 0.672145769622834 0.04842443914224087\n",
      "16 8 0.6703024125042474 0.05190265063456166\n",
      "16 9 0.672145769622834 0.05978109313495168\n",
      "16 10 0.6666411824668705 0.05861282297856242\n",
      "16 11 0.6675671083927964 0.05229986171966015\n",
      "16 12 0.6666496772001358 0.05545776446846677\n",
      "16 13 0.662962962962963 0.059220016574254936\n",
      "16 14 0.6602106693849813 0.05583122321939674\n",
      "16 15 0.6611365953109072 0.05718577776718872\n",
      "16 16 0.6647978253482841 0.055138763316945894\n",
      "16 17 0.6666326877336053 0.05527202758482633\n",
      "16 18 0.662037037037037 0.058234742284832254\n",
      "16 19 0.6620285423037716 0.05815244236441275\n",
      "17 1 0.7226129799524295 0.0521566135405212\n",
      "17 2 0.7033469249065579 0.05060217561818006\n",
      "17 3 0.7006031260618417 0.05408336231575933\n",
      "17 4 0.6914203194019708 0.046383175907146044\n",
      "17 5 0.6758154943934761 0.046317973054065785\n",
      "17 6 0.6739636425416242 0.04388406583303277\n",
      "17 7 0.6739721372748896 0.047843556439703186\n",
      "17 8 0.6721372748895685 0.058125521337439805\n",
      "17 9 0.672145769622834 0.0577763679915511\n",
      "17 10 0.6675671083927964 0.055728042608123786\n",
      "17 11 0.6666496772001359 0.05344835677165978\n",
      "17 12 0.6666411824668705 0.05626833407584587\n",
      "17 13 0.6611281005776419 0.05855193310516014\n",
      "17 14 0.662962962962963 0.05540159534306225\n",
      "17 15 0.6620455317703025 0.05641223359330603\n",
      "17 16 0.6684760448521917 0.053425052383372654\n",
      "17 17 0.6666326877336053 0.05435067096316122\n",
      "17 18 0.662037037037037 0.059237829197550336\n",
      "17 19 0.6601936799184506 0.05959949657155837\n",
      "18 1 0.7216955487597689 0.05121451789737975\n",
      "18 2 0.7033469249065579 0.0499324120970719\n",
      "18 3 0.6978423377505947 0.053855657591141935\n",
      "18 4 0.6895854570166496 0.045007174778835105\n",
      "18 5 0.6721542643560992 0.04603913777958517\n",
      "18 6 0.6730462113489637 0.04574567191953551\n",
      "18 7 0.6748810737342847 0.04702956309688129\n",
      "18 8 0.6721372748895685 0.058125521337439805\n",
      "18 9 0.6703024125042474 0.058891686482037506\n",
      "18 10 0.6648148148148149 0.05331111868120171\n",
      "18 11 0.6657322460074753 0.05186832935027428\n",
      "18 12 0.6684760448521917 0.0592515537088609\n",
      "18 13 0.6638803941556236 0.05618410159933637\n",
      "18 14 0.662962962962963 0.05792674653618178\n",
      "18 15 0.6629544682296975 0.055312391604131185\n",
      "18 16 0.6675586136595312 0.0534744808495028\n",
      "18 17 0.6684760448521917 0.055283265284804636\n",
      "18 18 0.6611196058443765 0.06058816380622846\n",
      "18 19 0.65927624872579 0.06077572928768921\n",
      "19 1 0.7216955487597689 0.05121451789737975\n",
      "19 2 0.7024294937138975 0.046820654917536716\n",
      "19 3 0.6960074753652735 0.051609852448951034\n",
      "19 4 0.6877505946313286 0.04389562980179804\n",
      "19 5 0.6721457696228338 0.048597941377242736\n",
      "19 6 0.6748895684675501 0.047310536668605685\n",
      "19 7 0.6721287801563032 0.046077864596706535\n",
      "19 8 0.6730547060822291 0.060563156581384336\n",
      "19 9 0.6703109072375127 0.05468199686440716\n",
      "19 10 0.6648148148148149 0.05121756121480081\n",
      "19 11 0.6666496772001358 0.05376238500402761\n",
      "19 12 0.6647978253482841 0.060662673984953605\n",
      "19 13 0.6647978253482841 0.057529303993421294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 14 0.662962962962963 0.05704828100017617\n",
      "19 15 0.6657152565409447 0.05343122128748275\n",
      "19 16 0.6684760448521917 0.053425052383372654\n",
      "19 17 0.6666411824668705 0.054443752821485614\n",
      "19 18 0.6602021746517159 0.060936132300456566\n",
      "19 19 0.662037037037037 0.06215002819959218\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "trainDataScaled = np.array(trainDataScaled)\n",
    "\n",
    "for i in range(10, 20):\n",
    "    \n",
    "    for j in range(1, 20):\n",
    "        \n",
    "        clfSVM = SVC(C = i, gamma = j)\n",
    "\n",
    "        accuracy_lst = []\n",
    "\n",
    "        for train_index, test_index in kf.split(trainDataScaled):\n",
    "\n",
    "            data_train = trainDataScaled[train_index]\n",
    "            data_test = trainDataScaled[test_index]\n",
    "\n",
    "            labels_train = labels[train_index]\n",
    "            labels_test = labels[test_index]\n",
    "            \n",
    "            clfSVM.fit(data_train, labels_train) \n",
    "\n",
    "            accuracy = np.sum(clfSVM.predict(data_test) == labels_test) * 1.0/len(data_test)\n",
    "            accuracy_lst.append(accuracy) \n",
    "\n",
    "        print i, j, np.mean(accuracy_lst), np.std(accuracy_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaling = MinMaxScaler(feature_range=(-1,1)).fit(trainData)\n",
    "trainDataScaled = scaling.transform(trainData)\n",
    "testDataScaled = scaling.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfSVM = SVC(kernel='rbf', probability=True, C = 14, gamma = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=14, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfSVM.fit(trainDataScaled, trainLabels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfSVM_probs = clfSVM.predict_proba(testDataScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.max(clfSVM_probs, axis = 1) < 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(clfSVM.predict(testData) != clfRF.predict(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raghav\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:699: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4699999999999999 0.05467073155618909\n",
      "1 0.55 0.11948965552623277\n",
      "2 0.5566666666666666 0.1653615567308328\n",
      "3 0.56 0.16983652270475993\n",
      "4 0.5633333333333334 0.17220788470785986\n",
      "5 0.5700000000000001 0.1702612371882951\n",
      "6 0.5766666666666667 0.17515072873892867\n",
      "7 0.5766666666666667 0.17515072873892867\n",
      "8 0.5766666666666667 0.17515072873892867\n",
      "9 0.5766666666666667 0.17515072873892867\n",
      "10 0.5766666666666667 0.17515072873892867\n",
      "11 0.5766666666666667 0.17515072873892867\n",
      "12 0.5766666666666667 0.17515072873892867\n",
      "13 0.5766666666666667 0.17515072873892867\n",
      "14 0.5766666666666667 0.17515072873892867\n",
      "15 0.5766666666666667 0.17515072873892867\n",
      "16 0.5766666666666667 0.17515072873892867\n",
      "17 0.5766666666666667 0.17515072873892867\n",
      "18 0.5766666666666667 0.17515072873892867\n",
      "19 0.5766666666666667 0.17515072873892867\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "\n",
    "for i in range(20):\n",
    "        \n",
    "    clfNB = MultinomialNB(alpha = i)\n",
    "            \n",
    "    accuracy_lst = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(data):\n",
    "    \n",
    "        data_train = data.iloc[train_index]\n",
    "        data_test = data.iloc[test_index]\n",
    "        \n",
    "        labels_train = labels[train_index]\n",
    "        labels_test = labels[test_index]\n",
    "        \n",
    "        # ---------------------------------------- #\n",
    "        \n",
    "        countVect = CountVectorizer()\n",
    "        TF_IDF_Transformer = TfidfTransformer()\n",
    "        \n",
    "        trainDataCounts = countVect.fit_transform(data_train.text)\n",
    "        trainDataTF_IDF = TF_IDF_Transformer.fit_transform(trainDataCounts)\n",
    "\n",
    "        clfNB.fit(trainDataTF_IDF, labels_train)\n",
    "        \n",
    "        # ---------------------------------------- #\n",
    "                        \n",
    "        testDataCounts = countVect.transform(data_test.text)\n",
    "        \n",
    "        testDataTF_IDF = TF_IDF_Transformer.transform(testDataCounts)\n",
    "        \n",
    "        # ---------------------------------------- #\n",
    "                               \n",
    "        accuracy = np.sum(clfNB.predict(testDataTF_IDF) == labels_test) * 1.0/len(data_test)\n",
    "        accuracy_lst.append(accuracy) \n",
    "        \n",
    "    print i, np.mean(accuracy_lst), np.std(accuracy_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVect = CountVectorizer()\n",
    "TF_IDF_Transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataText = pd.read_csv('train.csv').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataCounts = countVect.fit_transform(trainDataText)\n",
    "trainDataTF_IDF = TF_IDF_Transformer.fit_transform(trainDataCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=6, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNB = MultinomialNB(alpha = 6)\n",
    "clfNB.fit(trainDataTF_IDF, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataText = pd.read_csv('test.csv').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataCounts = countVect.transform(testDataText)\n",
    "testDataTF_IDF = TF_IDF_Transformer.transform(testDataCounts)\n",
    "clfNB_probs = clfNB.predict_proba(testDataTF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.max(clfNB_probs, axis = 1) < 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.sum(clfNB.predict(testDataTF_IDF) == clfRF.predict(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06 l1 0.4316173972137275 0.08261826648208546\n",
      "1e-06 l2 0.5683826027862725 0.08261826648208546\n",
      "1e-05 l1 0.4316173972137275 0.08261826648208546\n",
      "1e-05 l2 0.5647043832823649 0.08236471690431786\n",
      "0.0001 l1 0.5683826027862725 0.08261826648208546\n",
      "0.0001 l2 0.6436034658511723 0.0828396636286068\n",
      "0.001 l1 0.5665392456676861 0.08385288897080138\n",
      "0.001 l2 0.6683995922528032 0.09603079509316109\n",
      "0.01 l1 0.6693255181787292 0.09309754894790312\n",
      "0.01 l2 0.6785083248386001 0.08739867903274938\n",
      "0.1 l1 0.6766989466530752 0.07751906949070099\n",
      "0.1 l2 0.6886340468909277 0.06787354818443965\n",
      "1 l1 0.6932296975874956 0.06318297917562715\n",
      "1 l2 0.6923122663948351 0.06278368167427573\n",
      "10 l1 0.6968909276248727 0.0606571281101051\n",
      "10 l2 0.6959734964322122 0.060157071662847866\n",
      "100 l1 0.6968909276248727 0.0606571281101051\n",
      "100 l2 0.6968909276248727 0.0606571281101051\n",
      "1000 l1 0.6968909276248727 0.0606571281101051\n",
      "1000 l2 0.6968909276248727 0.0606571281101051\n",
      "10000 l1 0.6968909276248727 0.0606571281101051\n",
      "10000 l2 0.6968909276248727 0.0606571281101051\n",
      "100000 l1 0.6968909276248727 0.0606571281101051\n",
      "100000 l2 0.6968909276248727 0.0606571281101051\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "trainData = np.array(trainData)\n",
    "\n",
    "for i in range(-6, 6):\n",
    "    \n",
    "    for j in ['l1', 'l2']:\n",
    "        \n",
    "        clfLR = LogisticRegression(C = 10 ** i, penalty = j)\n",
    "\n",
    "        accuracy_lst = []\n",
    "\n",
    "        for train_index, test_index in kf.split(trainData):\n",
    "\n",
    "            data_train = trainData[train_index]\n",
    "            data_test = trainData[test_index]\n",
    "\n",
    "            labels_train = labels[train_index]\n",
    "            labels_test = labels[test_index]\n",
    "            \n",
    "            clfLR.fit(data_train, labels_train) \n",
    "\n",
    "            accuracy = np.sum(clfLR.predict(data_test) == labels_test) * 1.0/len(data_test)\n",
    "            accuracy_lst.append(accuracy) \n",
    "\n",
    "        print 10 ** i, j, np.mean(accuracy_lst), np.std(accuracy_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfLR = LogisticRegression(C = 10, penalty = 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfLR.fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfLR_probs = clfLR.predict_proba(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.max(clfLR_probs, axis = 1) < 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(trainData)  \n",
    "trainDataScaled = scaler.transform(trainData)  \n",
    "# apply same transformation to test data\n",
    "testDataScaled = scaler.transform(testData)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 5), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, 5))\n",
    "clfNN.fit(trainDataScaled, trainLabels)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfNN_probs = clfNN.predict_proba(testDataScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.max(clfNN_probs, axis = 1) < 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.max(clfNN_probs, axis = 1) == 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "predRF = np.max(clfRF_probs, axis = 1) * np.sign(np.argmax(clfRF_probs, axis = 1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "predSVM = np.max(clfSVM_probs, axis = 1) * np.sign(np.argmax(clfSVM_probs, axis = 1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "predNB = np.max(predicted, axis = 1) * np.sign(np.argmax(predicted, axis = 1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLR = np.max(clfLR_probs, axis = 1) * np.sign(np.argmax(clfLR_probs, axis = 1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predNN = np.max(clfNN_probs, axis = 1) * np.sign(np.argmax(clfNN_probs, axis = 1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightRF = np.log(np.max(clfRF_probs, axis = 1) / (1 - np.max(clfRF_probs, axis = 1)))\n",
    "# weightRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightSVM = np.log(np.max(clfSVM_probs, axis = 1) / (1 - np.max(clfSVM_probs, axis = 1)))\n",
    "# weightSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightNB = np.log(np.max(predicted, axis = 1) / (1 - np.max(predicted, axis = 1)))\n",
    "# weightNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightLR = np.log(np.max(clfLR_probs, axis = 1) / (1 - np.max(clfLR_probs, axis = 1)))\n",
    "# weightLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightNN = np.log(np.max(clfNN_probs, axis = 1) / (1 - np.max(clfNN_probs, axis = 1)))\n",
    "# weightNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedScore = weightNB * clfNB.predict(testDataTF_IDF) + weightRF * clfRF.predict(testData) + weightSVM * clfRF.predict(testDataScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "predFinalWeighted = np.sign(weightedScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predFinalWeighted.astype('int')).to_csv('NEW_NB_RF_SVM.csv', header=['Label'], index_label = ['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(clfRF.predict(testData) != clfSVM.predict(testDataScaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(clfNB.predict(testDataTF_IDF) != clfSVM.predict(testDataScaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(clfRF.predict(testData) != clfNB.predict(testDataTF_IDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(abs(clfRF.predict(testData) + clfNB.predict(testDataTF_IDF) + clfSVM.predict(testDataScaled)) == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "probWeightedScore = weightNB * clfNB.predict(testDataTF_IDF) * np.max(clfNB_probs, axis = 1) + weightRF * clfRF.predict(testData) * np.max(clfRF_probs, axis = 1) + weightSVM * clfSVM.predict(testDataScaled) * np.max(clfSVM_probs, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "predFinalProbWeighted = np.sign(probWeightedScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predFinalProbWeighted != predFinalWeighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predFinalProbWeighted.astype('int')).to_csv('NEW_NB_RF_SVM_PW.csv', header=['Label'], index_label = ['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedModel = pd.DataFrame({'RF': clfRF.predict_proba(trainData)[:, 0], 'NB': clfNB.predict_proba(trainDataTF_IDF)[:, 0], 'SVM': clfSVM.predict_proba(trainDataScaled)[:, 0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 l1 0.977055725450221 0.013752994144232221\n",
      "0.01 l2 0.9935694869181109 0.008259751140237595\n",
      "0.1 l1 1.0 0.0\n",
      "0.1 l2 1.0 0.0\n",
      "1 l1 1.0 0.0\n",
      "1 l2 1.0 0.0\n",
      "10 l1 1.0 0.0\n",
      "10 l2 1.0 0.0\n",
      "100 l1 1.0 0.0\n",
      "100 l2 1.0 0.0\n",
      "1000 l1 1.0 0.0\n",
      "1000 l2 1.0 0.0\n",
      "10000 l1 1.0 0.0\n",
      "10000 l2 1.0 0.0\n",
      "100000 l1 1.0 0.0\n",
      "100000 l2 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10)\n",
    "stackedModel = np.array(stackedModel)\n",
    "\n",
    "for i in range(-2, 6):\n",
    "    \n",
    "    for j in ['l1', 'l2']:\n",
    "        \n",
    "        clfStack = LogisticRegression(C = 10 ** i, penalty = j)\n",
    "\n",
    "        accuracy_lst = []\n",
    "\n",
    "        for train_index, test_index in kf.split(stackedModel):\n",
    "\n",
    "            data_train = stackedModel[train_index]\n",
    "            data_test = stackedModel[test_index]\n",
    "\n",
    "            labels_train = labels[train_index]\n",
    "            labels_test = labels[test_index]\n",
    "            \n",
    "            clfStack.fit(data_train, labels_train) \n",
    "\n",
    "            accuracy = np.sum(clfStack.predict(data_test) == labels_test) * 1.0/len(data_test)\n",
    "            accuracy_lst.append(accuracy) \n",
    "\n",
    "        print 10 ** i, j, np.mean(accuracy_lst), np.std(accuracy_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfStack = LogisticRegression(C = 1, penalty = 'l1')\n",
    "clfStack.fit(stackedModel, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedModelPredict = pd.DataFrame({'RF': clfRF.predict_proba(testData)[:, 0], 'NB':  clfNB.predict_proba(testDataTF_IDF)[:, 0], 'SVM': clfSVM.predict_proba(testDataScaled)[:, 0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "predStack = clfStack.predict(stackedModelPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predStack.astype('int')).to_csv('NEW PRED STACK.csv', header=['Label'], index_label = ['ID'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
